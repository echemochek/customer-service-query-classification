{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Service Query Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libaries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Customer_Service_Questions_Multiclass.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi! If I sign up for your email list, can I se...</td>\n",
       "      <td>Sales/Promotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going to be out of the country for about a...</td>\n",
       "      <td>Shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was wondering if you'd be able to overnight ...</td>\n",
       "      <td>Shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Swingline electronic stapler (472555) look...</td>\n",
       "      <td>Shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I think this cosmetic bag would work great for...</td>\n",
       "      <td>Shipping</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question             topic\n",
       "0  Hi! If I sign up for your email list, can I se...  Sales/Promotions\n",
       "1  I'm going to be out of the country for about a...          Shipping\n",
       "2  I was wondering if you'd be able to overnight ...          Shipping\n",
       "3  The Swingline electronic stapler (472555) look...          Shipping\n",
       "4  I think this cosmetic bag would work great for...          Shipping"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "# missingno.matrix(df) # there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# distribution of the topics column\n",
    "sns.countplot(x=df[\"topic\"])\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=65)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# encoding the labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['category'] = encoder.fit_transform(df['topic'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(df.question, df.topic, stratify=df.topic, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data in different formats\n",
    "\n",
    "1. TF-IDF vector\n",
    "2. TF-IDF vector of n-grams\n",
    "3. Word vectors (GloVe)\n",
    "4. Document vectors (Doc2Vec)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "                            strip_accents=\"unicode\", lowercase=True, analyzer='word', \n",
    "                            stop_words='english', max_df=0.95, min_df=0.05, max_features=500\n",
    "                            )\n",
    "vectorizer.fit(df.question)\n",
    "\n",
    "dfTfidf_train = vectorizer.transform(xtrain)\n",
    "dfTfidf_test = vectorizer.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. TF-IDF n-grams vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vector of n-grams\n",
    "ngrams_vectorizer = TfidfVectorizer(\n",
    "                            strip_accents=\"unicode\", lowercase=True, analyzer='word', ngram_range=(2,3), \n",
    "                            max_df=0.95, min_df=0.05, max_features=500\n",
    "                            )\n",
    "ngrams_vectorizer.fit(df.question)\n",
    "\n",
    "dfTfidf_ngrams_train = ngrams_vectorizer.transform(xtrain)\n",
    "dfTfidf_ngrams_test = ngrams_vectorizer.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (4000, 100) \n",
      "Test features shape: (1000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Word vectors \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "wordvec = Word2Vec(xtrain, window=8, min_count=2, sample=1e-3, sg=1, workers=8)\n",
    "vocab = set(wordvec.wv.index_to_key)\n",
    "\n",
    "num_features = 100\n",
    "\n",
    "def average_word_vectors(tokens, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    ntokens = 0.\n",
    "    for t in tokens:\n",
    "        if t in vocabulary: \n",
    "            ntokens = ntokens + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[t])\n",
    "    if ntokens:\n",
    "        feature_vector = np.divide(feature_vector, ntokens)\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "word2vec_train = [average_word_vectors(sent_tokens, wordvec, vocab, num_features) \n",
    "               for sent_tokens in xtrain]\n",
    "avg_word2vec_train = np.array(word2vec_train)\n",
    "\n",
    "word2vec_test = [average_word_vectors(sent_tokens, wordvec, vocab, num_features) \n",
    "              for sent_tokens in xtest]\n",
    "avg_word2vec_test = np.array(word2vec_test)\n",
    "\n",
    "print('Train features shape:', avg_word2vec_train.shape, \n",
    "      '\\nTest features shape:', avg_word2vec_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vector\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(xtrain)]\n",
    "docvec = Doc2Vec(vector_size=100, window=3, min_count=4, workers=4, epochs=40)\n",
    "docvec.build_vocab(docs)\n",
    "docvec.train(docs, total_examples=docvec.corpus_count, epochs=docvec.epochs)\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "xtrainTokenized = [simple_preprocess(h) for h in xtrain]\n",
    "xtestTokenized = [simple_preprocess(h) for h in xtest]\n",
    "\n",
    "docvec_train = [docvec.infer_vector(i) for i in xtrainTokenized]\n",
    "docvec_test =  [docvec.infer_vector(i) for i in xtestTokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models to consider:\n",
    "-\n",
    "1. One multi-class classifier (e.g., Naive Bayes, Logistic, Decision Tree, SVM)\n",
    "2. One ensemble classifier whose code is also provided (e.g., Random Forest, XGBoost)\n",
    "3. One other model of your choice whose code is NOT provided in class handouts\n",
    "\n",
    "Input features to consider for each model:\n",
    "-\n",
    "1. TF-IDF vector of tokenized words\n",
    "2. TF-IDF vector of n-grams (of range 4-5)\n",
    "3. Word vectors (Glove, Word2Vec, or FastText)\n",
    "4. Document vectors (Doc2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "model_SVC_tfidf = svc.fit(dfTfidf_train, ytrain)\n",
    "svc_tfidf_pred = model_SVC_tfidf.predict(dfTfidf_test)\n",
    "\n",
    "# n-grams tf-idf\n",
    "model_SVC_ngramTfidf = svc.fit(dfTfidf_ngrams_train, ytrain)\n",
    "svc_ngram_tfidf_pred = model_SVC_ngramTfidf.predict(dfTfidf_ngrams_test)\n",
    "\n",
    "# word vectors\n",
    "model_word2vec = svc.fit(avg_word2vec_train, ytrain)\n",
    "svc_word2vec_pred = model_word2vec.predict(avg_word2vec_test)\n",
    "\n",
    "# document vectors\n",
    "model_doc2vec = svc.fit(docvec_train, ytrain)\n",
    "svc_doc2vec_pred = model_doc2vec.predict(docvec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Omnichannel       0.90      0.83      0.87        90\n",
      "  Product Availability       0.53      0.83      0.65       167\n",
      "    Product Comparison       0.69      0.49      0.57       161\n",
      "Product Specifications       0.67      0.65      0.66       168\n",
      "     Returns & Refunds       0.98      0.97      0.98       153\n",
      "      Sales/Promotions       0.70      0.49      0.57       101\n",
      "              Shipping       0.93      0.92      0.92       160\n",
      "\n",
      "              accuracy                           0.75      1000\n",
      "             macro avg       0.77      0.74      0.75      1000\n",
      "          weighted avg       0.76      0.75      0.75      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# svc tf-idf\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nTF-IDF\\n\", classification_report(ytest, svc_tfidf_pred)) # not removing stopwords improves the model significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF n-grams\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Omnichannel       0.85      0.74      0.79        90\n",
      "  Product Availability       0.81      0.89      0.85       167\n",
      "    Product Comparison       0.88      0.66      0.76       161\n",
      "Product Specifications       0.60      0.83      0.69       168\n",
      "     Returns & Refunds       0.78      0.82      0.80       153\n",
      "      Sales/Promotions       0.77      0.48      0.59       101\n",
      "              Shipping       0.85      0.84      0.85       160\n",
      "\n",
      "              accuracy                           0.77      1000\n",
      "             macro avg       0.79      0.75      0.76      1000\n",
      "          weighted avg       0.79      0.77      0.77      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# svc n-grams tf-idf\n",
    "print(\"\\nTF-IDF n-grams\\n\", classification_report(ytest, svc_ngram_tfidf_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word vectors\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Omnichannel       0.86      0.07      0.12        90\n",
      "  Product Availability       0.48      0.87      0.62       167\n",
      "    Product Comparison       0.74      0.50      0.59       161\n",
      "Product Specifications       0.47      0.46      0.46       168\n",
      "     Returns & Refunds       0.54      0.58      0.56       153\n",
      "      Sales/Promotions       0.65      0.11      0.19       101\n",
      "              Shipping       0.51      0.76      0.61       160\n",
      "\n",
      "              accuracy                           0.53      1000\n",
      "             macro avg       0.61      0.48      0.45      1000\n",
      "          weighted avg       0.59      0.53      0.49      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word vectors\n",
    "print(\"\\nWord vectors\\n\", classification_report(ytest, svc_word2vec_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Vectors\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Omnichannel       1.00      0.16      0.27        90\n",
      "  Product Availability       0.91      0.58      0.71       167\n",
      "    Product Comparison       0.21      0.32      0.25       161\n",
      "Product Specifications       0.22      0.36      0.28       168\n",
      "     Returns & Refunds       0.41      0.36      0.38       153\n",
      "      Sales/Promotions       0.96      0.44      0.60       101\n",
      "              Shipping       0.26      0.29      0.27       160\n",
      "\n",
      "              accuracy                           0.37      1000\n",
      "             macro avg       0.57      0.36      0.39      1000\n",
      "          weighted avg       0.51      0.37      0.39      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# document vectors\n",
    "print(\"\\nDocument Vectors\\n\", classification_report(ytest, svc_doc2vec_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfe9ce709e982859ebd8c1b094ee35d9f73a27801040ad55cc46450c9d5cadda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
